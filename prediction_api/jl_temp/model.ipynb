{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a86ca26e",
   "metadata": {},
   "source": [
    "# Route Efficiency Prediction Model - Refactored\n",
    "\n",
    "**Objective**: Train a predictive model on `route_efficiency` using trip-level data + aggregated telemetry from rows.csv\n",
    "\n",
    "**Key improvements**:\n",
    "- Uses all three CSVs (trips, rows aggregates, combined if useful)\n",
    "- Strict leakage controls (no target components as features)\n",
    "- Explainability-first feature design\n",
    "- API-ready artifacts for Flask/FastAPI deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cd8e66",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.inspection import permutation_importance\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ Libraries imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2675424e",
   "metadata": {},
   "source": [
    "## 1. Load and Audit Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e31ed2b",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Load trips (base table: one row per trip)\n",
    "trips = pd.read_csv('data/safetruck_data_iter1_trips.csv')\n",
    "\n",
    "# Load rows (telemetry: many rows per trip)\n",
    "rows = pd.read_csv('data/safetruck_data_iter1_rows.csv')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DATA AUDIT\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Trips: {len(trips):,} rows, {len(trips.columns)} columns\")\n",
    "print(f\"  Columns: {list(trips.columns)}\")\n",
    "print(f\"\\nRows (telemetry): {len(rows):,} rows, {len(rows.columns)} columns\")\n",
    "print(f\"  Columns: {list(rows.columns)}\")\n",
    "print(f\"\\nUnique trips in trips.csv: {trips['trip_id'].nunique():,}\")\n",
    "print(f\"Unique trips in rows.csv: {rows['trip_id'].nunique():,}\")\n",
    "print(f\"Unique vehicles in trips: {trips['vehicle_id'].nunique():,}\")\n",
    "print(f\"Unique vehicles in rows: {rows['vehicle_id'].nunique():,}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0ee48b",
   "metadata": {},
   "source": [
    "## 2. Aggregate Row-Level Telemetry (Per Trip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad0130d",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Compute robust telemetry aggregates per trip_id\n",
    "# These capture driving behavior without directly encoding the target\n",
    "\n",
    "print(\"Aggregating row-level telemetry...\")\n",
    "\n",
    "# Speed profile aggregates\n",
    "speed_aggs = rows.groupby('trip_id')['speed_kmh'].agg([\n",
    "    ('speed_mean', 'mean'),\n",
    "    ('speed_median', 'median'),\n",
    "    ('speed_p25', lambda x: x.quantile(0.25)),\n",
    "    ('speed_p75', lambda x: x.quantile(0.75)),\n",
    "    ('speed_p90', lambda x: x.quantile(0.90)),\n",
    "    ('speed_std', 'std'),\n",
    "    ('speed_min', 'min'),\n",
    "    ('speed_max', 'max')\n",
    "]).reset_index()\n",
    "\n",
    "# Speed variability\n",
    "speed_aggs['speed_iqr'] = speed_aggs['speed_p75'] - speed_aggs['speed_p25']\n",
    "speed_aggs['speed_cv'] = speed_aggs['speed_std'] / (speed_aggs['speed_mean'] + 1e-6)\n",
    "\n",
    "# Stop/idle behavior (speed < 3 km/h threshold)\n",
    "stop_threshold = 3.0\n",
    "rows['is_stopped'] = rows['speed_kmh'] < stop_threshold\n",
    "\n",
    "stop_aggs = rows.groupby('trip_id').agg(\n",
    "    stop_count=('is_stopped', 'sum'),\n",
    "    total_rows=('is_stopped', 'count')\n",
    ").reset_index()\n",
    "\n",
    "# Overspeeding (speed > 90 km/h as example threshold)\n",
    "overspeed_threshold = 90.0\n",
    "rows['is_overspeeding'] = rows['speed_kmh'] > overspeed_threshold\n",
    "overspeed_aggs = rows.groupby('trip_id')['is_overspeeding'].sum().reset_index()\n",
    "overspeed_aggs.columns = ['trip_id', 'overspeed_count']\n",
    "\n",
    "# Idle behavior (from is_idle flag)\n",
    "idle_aggs = rows.groupby('trip_id')['is_idle'].agg([\n",
    "    ('idle_event_count', 'sum'),\n",
    "    ('idle_percentage_rows', 'mean')\n",
    "]).reset_index()\n",
    "\n",
    "# Moving behavior\n",
    "moving_aggs = rows.groupby('trip_id')['is_moving'].agg([\n",
    "    ('moving_event_count', 'sum'),\n",
    "    ('moving_percentage_rows', 'mean')\n",
    "]).reset_index()\n",
    "\n",
    "# Merge all aggregates\n",
    "telemetry_aggs = speed_aggs\n",
    "telemetry_aggs = telemetry_aggs.merge(stop_aggs, on='trip_id', how='left')\n",
    "telemetry_aggs = telemetry_aggs.merge(overspeed_aggs, on='trip_id', how='left')\n",
    "telemetry_aggs = telemetry_aggs.merge(idle_aggs, on='trip_id', how='left')\n",
    "telemetry_aggs = telemetry_aggs.merge(moving_aggs, on='trip_id', how='left')\n",
    "\n",
    "# Fill NaN with 0 for counts\n",
    "telemetry_aggs = telemetry_aggs.fillna(0)\n",
    "\n",
    "print(f\"✓ Aggregated {len(telemetry_aggs):,} trips\")\n",
    "print(f\"  Aggregated features: {len(telemetry_aggs.columns) - 1}\")\n",
    "print(telemetry_aggs.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba1853d",
   "metadata": {},
   "source": [
    "## 3. Merge Trips + Telemetry Aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006a47da",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Join trips with telemetry aggregates\n",
    "df = trips.merge(telemetry_aggs, on='trip_id', how='left')\n",
    "\n",
    "# Normalize telemetry features per distance (per 100km) and per time (per hour)\n",
    "epsilon = 1e-6\n",
    "\n",
    "# Per 100km rates\n",
    "df['stop_count_per_100km'] = df['stop_count'] / (df['Trip_Distance_km'] / 100 + epsilon)\n",
    "df['overspeed_count_per_100km'] = df['overspeed_count'] / (df['Trip_Distance_km'] / 100 + epsilon)\n",
    "df['idle_events_per_100km'] = df['idle_event_count'] / (df['Trip_Distance_km'] / 100 + epsilon)\n",
    "\n",
    "# Per hour rates\n",
    "df['stop_count_per_hour'] = df['stop_count'] / (df['Trip_Duration_min'] / 60 + epsilon)\n",
    "df['overspeed_count_per_hour'] = df['overspeed_count'] / (df['Trip_Duration_min'] / 60 + epsilon)\n",
    "\n",
    "print(f\"✓ Combined dataset: {len(df):,} rows, {len(df.columns)} columns\")\n",
    "print(f\"\\nSample of normalized features:\")\n",
    "print(df[['trip_id', 'Trip_Distance_km', 'stop_count', 'stop_count_per_100km', 'overspeed_count_per_100km']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa1a5f3",
   "metadata": {},
   "source": [
    "## 4. QA Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40c2031",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Convert timestamps\n",
    "df['timestamp_start'] = pd.to_datetime(df['timestamp_start'])\n",
    "df['timestamp_end'] = pd.to_datetime(df['timestamp_end'])\n",
    "\n",
    "# Apply strict QA filters\n",
    "print(f\"Initial trips: {len(df):,}\")\n",
    "\n",
    "df_clean = df[\n",
    "    (df['QA_zero_duration'] == False) &\n",
    "    (df['QA_negative_distance'] == False) &\n",
    "    (df['Trip_Duration_min'] > 0) &\n",
    "    (df['Trip_Distance_km'] > 0) &\n",
    "    (df['lat_start'].notna()) &\n",
    "    (df['lon_start'].notna()) &\n",
    "    (df['lat_end'].notna()) &\n",
    "    (df['lon_end'].notna())\n",
    "].copy()\n",
    "\n",
    "print(f\"After QA filters: {len(df_clean):,}\")\n",
    "print(f\"Removed: {len(df) - len(df_clean):,} trips ({100*(len(df) - len(df_clean))/len(df):.1f}%)\")\n",
    "print(\"✓ QA filtering complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fd4a6d",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering (Leakage-Safe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e712ae91",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Calculate great-circle distance in km\"\"\"\n",
    "    R = 6371  # Earth radius in km\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    return R * c\n",
    "\n",
    "def calculate_bearing(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Calculate bearing in degrees\"\"\"\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlon = lon2 - lon1\n",
    "    x = np.sin(dlon) * np.cos(lat2)\n",
    "    y = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(dlon)\n",
    "    return np.degrees(np.arctan2(x, y))\n",
    "\n",
    "# Geographic features\n",
    "df_clean['haversine_km'] = haversine(\n",
    "    df_clean['lat_start'], df_clean['lon_start'],\n",
    "    df_clean['lat_end'], df_clean['lon_end']\n",
    ")\n",
    "df_clean['bearing'] = calculate_bearing(\n",
    "    df_clean['lat_start'], df_clean['lon_start'],\n",
    "    df_clean['lat_end'], df_clean['lon_end']\n",
    ")\n",
    "df_clean['delta_lat'] = df_clean['lat_end'] - df_clean['lat_start']\n",
    "df_clean['delta_lon'] = df_clean['lon_end'] - df_clean['lon_start']\n",
    "\n",
    "# Time features (cyclical encoding)\n",
    "df_clean['hour_of_day'] = df_clean['timestamp_start'].dt.hour\n",
    "df_clean['day_of_week'] = df_clean['timestamp_start'].dt.dayofweek\n",
    "df_clean['hour_sin'] = np.sin(2 * np.pi * df_clean['hour_of_day'] / 24)\n",
    "df_clean['hour_cos'] = np.cos(2 * np.pi * df_clean['hour_of_day'] / 24)\n",
    "df_clean['dow_sin'] = np.sin(2 * np.pi * df_clean['day_of_week'] / 7)\n",
    "df_clean['dow_cos'] = np.cos(2 * np.pi * df_clean['day_of_week'] / 7)\n",
    "\n",
    "# Distance bucket\n",
    "df_clean['distance_bucket'] = pd.cut(\n",
    "    df_clean['Trip_Distance_km'],\n",
    "    bins=[0, 10, 50, 200, float('inf')],\n",
    "    labels=['0-10km', '10-50km', '50-200km', '200+km']\n",
    ")\n",
    "\n",
    "print(\"✓ Geographic and time features engineered\")\n",
    "print(df_clean[['haversine_km', 'bearing', 'hour_sin', 'hour_cos', 'distance_bucket']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4a14e5",
   "metadata": {},
   "source": [
    "## 6. Define Target Variable (route_efficiency)\n",
    "\n",
    "**CRITICAL**: We compute route_efficiency as our target but exclude its components from features to avoid leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c889239",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Use the pre-computed efficiency columns as components\n",
    "# These exist in the trips CSV but will NOT be used as features\n",
    "\n",
    "# Check if route_efficiency already exists in the data\n",
    "if 'route_efficiency' not in df_clean.columns:\n",
    "    # Compute composite efficiency (weighted combination)\n",
    "    # directness = haversine / actual_distance\n",
    "    # idle_efficiency = 1 - (idle_time / duration)\n",
    "    # We'll use the provided efficiency columns if available\n",
    "    \n",
    "    directness_eff = np.clip(df_clean['haversine_km'] / (df_clean['Trip_Distance_km'] + epsilon), 0, 1)\n",
    "    idle_eff = 1 - (df_clean['Idle_Time_min'] / (df_clean['Trip_Duration_min'] + epsilon))\n",
    "    \n",
    "    # Composite: 50% directness, 30% idle, 20% speed (relative to trip average)\n",
    "    # For simplicity, normalize speed against median for that distance bucket\n",
    "    df_clean['route_efficiency'] = (\n",
    "        0.5 * directness_eff +\n",
    "        0.3 * idle_eff +\n",
    "        0.2 * np.clip(df_clean['Avg_Speed'] / 60, 0, 1)  # normalize to ~60 km/h\n",
    "    )\n",
    "else:\n",
    "    print(\"Using existing route_efficiency column\")\n",
    "\n",
    "print(f\"\\nTarget variable (route_efficiency) distribution:\")\n",
    "print(df_clean['route_efficiency'].describe())\n",
    "print(f\"\\nCorrelation with provided efficiency metrics:\")\n",
    "corr_cols = ['route_efficiency', 'Distance_Efficiency', 'Time_Efficiency', 'Idle_Efficiency']\n",
    "print(df_clean[corr_cols].corr()['route_efficiency'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4028c86",
   "metadata": {},
   "source": [
    "## 7. Time-Based Train/Test Split (80/20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2859e35b",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Sort by time and split\n",
    "df_clean = df_clean.sort_values('timestamp_start').reset_index(drop=True)\n",
    "\n",
    "split_idx = int(len(df_clean) * 0.8)\n",
    "train_data = df_clean.iloc[:split_idx].copy()\n",
    "test_data = df_clean.iloc[split_idx:].copy()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TRAIN/TEST SPLIT\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Train: {len(train_data):,} trips\")\n",
    "print(f\"  Period: {train_data['timestamp_start'].min()} to {train_data['timestamp_start'].max()}\")\n",
    "print(f\"\\nTest: {len(test_data):,} trips\")\n",
    "print(f\"  Period: {test_data['timestamp_start'].min()} to {test_data['timestamp_start'].max()}\")\n",
    "print(f\"\\nVehicles in train: {train_data['vehicle_id'].nunique()}\")\n",
    "print(f\"Vehicles in test: {test_data['vehicle_id'].nunique()}\")\n",
    "print(f\"Overlap: {len(set(train_data['vehicle_id']) & set(test_data['vehicle_id']))}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e4f3c4",
   "metadata": {},
   "source": [
    "## 8. Zone Clustering (Train-Fit Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6242f773",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "k = 50  # Number of zones\n",
    "\n",
    "# Fit KMeans on training data only\n",
    "print(\"Fitting KMeans for origin zones...\")\n",
    "kmeans_start = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "train_data['zone_start_id'] = kmeans_start.fit_predict(train_data[['lat_start', 'lon_start']])\n",
    "test_data['zone_start_id'] = kmeans_start.predict(test_data[['lat_start', 'lon_start']])\n",
    "\n",
    "print(\"Fitting KMeans for destination zones...\")\n",
    "kmeans_end = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "train_data['zone_end_id'] = kmeans_end.fit_predict(train_data[['lat_end', 'lon_end']])\n",
    "test_data['zone_end_id'] = kmeans_end.predict(test_data[['lat_end', 'lon_end']])\n",
    "\n",
    "print(f\"✓ Created {k} origin zones and {k} destination zones\")\n",
    "print(f\"  Train zone_start_id range: {train_data['zone_start_id'].min()}-{train_data['zone_start_id'].max()}\")\n",
    "print(f\"  Train zone_end_id range: {train_data['zone_end_id'].min()}-{train_data['zone_end_id'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fa58ec",
   "metadata": {},
   "source": [
    "## 9. Feature Selection (Leakage-Safe)\n",
    "\n",
    "**Excluded to prevent leakage**:\n",
    "- `Distance_Efficiency`, `Time_Efficiency`, `Idle_Efficiency` (direct target components)\n",
    "- Any derived features that are exact transformations of the target formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1858f886",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Numeric features (no leakage)\n",
    "numeric_features = [\n",
    "    # Trip metrics (raw inputs)\n",
    "    'Trip_Distance_km', 'Trip_Duration_min', 'Avg_Speed',\n",
    "    'Idle_Time_min', 'Moving_Time_min', 'Idle_Percentage',\n",
    "    \n",
    "    # Geographic\n",
    "    'haversine_km', 'bearing', 'delta_lat', 'delta_lon',\n",
    "    'lat_start', 'lon_start', 'lat_end', 'lon_end',\n",
    "    \n",
    "    # Time (cyclical)\n",
    "    'hour_sin', 'hour_cos', 'dow_sin', 'dow_cos',\n",
    "    \n",
    "    # Vehicle metadata\n",
    "    'Age_of_Truck_months',\n",
    "    \n",
    "    # Telemetry aggregates (from rows.csv)\n",
    "    'speed_mean', 'speed_median', 'speed_p25', 'speed_p75', 'speed_p90',\n",
    "    'speed_std', 'speed_iqr', 'speed_cv',\n",
    "    'stop_count', 'stop_count_per_100km', 'stop_count_per_hour',\n",
    "    'overspeed_count', 'overspeed_count_per_100km', 'overspeed_count_per_hour',\n",
    "    'idle_event_count', 'idle_percentage_rows', 'idle_events_per_100km',\n",
    "    'moving_event_count', 'moving_percentage_rows'\n",
    "]\n",
    "\n",
    "# Categorical features\n",
    "categorical_features = ['vehicle_id', 'zone_start_id', 'zone_end_id', 'distance_bucket']\n",
    "\n",
    "# Filter to only features that exist in both train and test\n",
    "numeric_features = [f for f in numeric_features if f in train_data.columns and f in test_data.columns]\n",
    "categorical_features = [f for f in categorical_features if f in train_data.columns and f in test_data.columns]\n",
    "\n",
    "# Fill NaN in numeric features\n",
    "for feat in numeric_features:\n",
    "    train_data[feat] = train_data[feat].fillna(0)\n",
    "    test_data[feat] = test_data[feat].fillna(0)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FEATURE SELECTION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Numeric features ({len(numeric_features)}): \")\n",
    "for f in numeric_features:\n",
    "    print(f\"  - {f}\")\n",
    "print(f\"\\nCategorical features ({len(categorical_features)}): {categorical_features}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfc29ca",
   "metadata": {},
   "source": [
    "## 10. Target Encoding for vehicle_id (Train-Fit Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca60f8b",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Target encode vehicle_id on training data\n",
    "vehicle_means = train_data.groupby('vehicle_id')['route_efficiency'].mean()\n",
    "global_mean = train_data['route_efficiency'].mean()\n",
    "\n",
    "# Apply to train and test\n",
    "train_data['vehicle_id_encoded'] = train_data['vehicle_id'].map(vehicle_means).fillna(global_mean)\n",
    "test_data['vehicle_id_encoded'] = test_data['vehicle_id'].map(vehicle_means).fillna(global_mean)\n",
    "\n",
    "print(f\"✓ Target encoded vehicle_id using {len(vehicle_means)} unique vehicles\")\n",
    "print(f\"  Global mean route_efficiency: {global_mean:.4f}\")\n",
    "print(f\"  Unknown vehicles in test will use global mean\")\n",
    "\n",
    "# Update feature lists\n",
    "numeric_features.append('vehicle_id_encoded')\n",
    "categorical_features.remove('vehicle_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59be07a2",
   "metadata": {},
   "source": [
    "## 11. Prepare X and y for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690fda67",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# One-hot encode categorical features\n",
    "train_encoded = pd.get_dummies(train_data, columns=categorical_features, prefix=categorical_features)\n",
    "test_encoded = pd.get_dummies(test_data, columns=categorical_features, prefix=categorical_features)\n",
    "\n",
    "# Get dummy column names\n",
    "dummy_cols_train = [col for col in train_encoded.columns if any(col.startswith(f\"{cat}_\") for cat in categorical_features)]\n",
    "dummy_cols_test = [col for col in test_encoded.columns if any(col.startswith(f\"{cat}_\") for cat in categorical_features)]\n",
    "\n",
    "train_encoded = train_encoded[dummy_cols_train]\n",
    "test_encoded = test_encoded[dummy_cols_test]\n",
    "\n",
    "# Align test to train columns\n",
    "test_encoded = test_encoded.reindex(columns=train_encoded.columns, fill_value=0)\n",
    "\n",
    "# Combine numeric + categorical\n",
    "X_train = pd.concat([\n",
    "    train_data[numeric_features].reset_index(drop=True),\n",
    "    train_encoded.reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "X_test = pd.concat([\n",
    "    test_data[numeric_features].reset_index(drop=True),\n",
    "    test_encoded.reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "y_train = train_data['route_efficiency'].values\n",
    "y_test = test_data['route_efficiency'].values\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FEATURE MATRICES\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "print(f\"\\nTotal features: {X_train.shape[1]}\")\n",
    "print(f\"  Numeric: {len(numeric_features)}\")\n",
    "print(f\"  Categorical (one-hot): {len(train_encoded.columns)}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcf7b09",
   "metadata": {},
   "source": [
    "## 12. Baseline Model: Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0697d3c8",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Scale for linear model\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train Ridge\n",
    "ridge = Ridge(alpha=1.0, random_state=42)\n",
    "ridge.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_ridge = ridge.predict(X_train_scaled)\n",
    "y_test_pred_ridge = ridge.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate\n",
    "train_mae_ridge = mean_absolute_error(y_train, y_train_pred_ridge)\n",
    "train_rmse_ridge = np.sqrt(mean_squared_error(y_train, y_train_pred_ridge))\n",
    "train_r2_ridge = r2_score(y_train, y_train_pred_ridge)\n",
    "\n",
    "test_mae_ridge = mean_absolute_error(y_test, y_test_pred_ridge)\n",
    "test_rmse_ridge = np.sqrt(mean_squared_error(y_test, y_test_pred_ridge))\n",
    "test_r2_ridge = r2_score(y_test, y_test_pred_ridge)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"RIDGE REGRESSION RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Train - MAE: {train_mae_ridge:.4f}, RMSE: {train_rmse_ridge:.4f}, R²: {train_r2_ridge:.4f}\")\n",
    "print(f\"Test  - MAE: {test_mae_ridge:.4f}, RMSE: {test_rmse_ridge:.4f}, R²: {test_r2_ridge:.4f}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42431306",
   "metadata": {},
   "source": [
    "## 13. Main Model: LightGBM Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a7a93e",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Train LightGBM\n",
    "lgbm = lgb.LGBMRegressor(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=7,\n",
    "    num_leaves=31,\n",
    "    min_child_samples=20,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "lgbm.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    eval_metric='mae',\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=30, verbose=False)]\n",
    ")\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_lgbm = lgbm.predict(X_train)\n",
    "y_test_pred_lgbm = lgbm.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "train_mae_lgbm = mean_absolute_error(y_train, y_train_pred_lgbm)\n",
    "train_rmse_lgbm = np.sqrt(mean_squared_error(y_train, y_train_pred_lgbm))\n",
    "train_r2_lgbm = r2_score(y_train, y_train_pred_lgbm)\n",
    "\n",
    "test_mae_lgbm = mean_absolute_error(y_test, y_test_pred_lgbm)\n",
    "test_rmse_lgbm = np.sqrt(mean_squared_error(y_test, y_test_pred_lgbm))\n",
    "test_r2_lgbm = r2_score(y_test, y_test_pred_lgbm)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"LIGHTGBM RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Train - MAE: {train_mae_lgbm:.4f}, RMSE: {train_rmse_lgbm:.4f}, R²: {train_r2_lgbm:.4f}\")\n",
    "print(f\"Test  - MAE: {test_mae_lgbm:.4f}, RMSE: {test_rmse_lgbm:.4f}, R²: {test_r2_lgbm:.4f}\")\n",
    "print(f\"\\nBest iteration: {lgbm.best_iteration_}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063f1e08",
   "metadata": {},
   "source": [
    "## 14. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18eefa6e",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Get LightGBM feature importances\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': lgbm.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TOP 30 MOST IMPORTANT FEATURES (LightGBM Gain)\")\n",
    "print(\"=\" * 80)\n",
    "print(feature_importance.head(30).to_string(index=False))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Identify top features by type\n",
    "print(\"\\nTop features by category:\")\n",
    "print(\"\\n1. Top telemetry/behavior features:\")\n",
    "telemetry_feats = feature_importance[feature_importance['feature'].str.contains('speed|stop|idle|overspeed|moving')]\n",
    "print(telemetry_feats.head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\n2. Top geographic features:\")\n",
    "geo_feats = feature_importance[feature_importance['feature'].str.contains('lat|lon|haversine|bearing|delta|zone')]\n",
    "print(geo_feats.head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\n3. Top time features:\")\n",
    "time_feats = feature_importance[feature_importance['feature'].str.contains('hour|dow')]\n",
    "print(time_feats.head(5).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c9e6df",
   "metadata": {},
   "source": [
    "## 15. Model Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acac8e88",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Summary comparison\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['Ridge Regression', 'LightGBM'],\n",
    "    'Train MAE': [train_mae_ridge, train_mae_lgbm],\n",
    "    'Test MAE': [test_mae_ridge, test_mae_lgbm],\n",
    "    'Train RMSE': [train_rmse_ridge, train_rmse_lgbm],\n",
    "    'Test RMSE': [test_rmse_ridge, test_rmse_lgbm],\n",
    "    'Train R²': [train_r2_ridge, train_r2_lgbm],\n",
    "    'Test R²': [test_r2_ridge, test_r2_lgbm]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(results.to_string(index=False))\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nBest model by Test MAE: {results.loc[results['Test MAE'].idxmin(), 'Model']}\")\n",
    "print(f\"Best model by Test R²: {results.loc[results['Test R²'].idxmax(), 'Model']}\")\n",
    "print(f\"\\nImprovement of LightGBM over Ridge:\")\n",
    "print(f\"  MAE: {(test_mae_ridge - test_mae_lgbm) / test_mae_ridge * 100:.1f}% better\")\n",
    "print(f\"  R²: {(test_r2_lgbm - test_r2_ridge) / (1 - test_r2_ridge) * 100:.1f}% variance explained improvement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cbcbc8",
   "metadata": {},
   "source": [
    "## 16. Package Model Artifacts for API Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a8666b",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Create models directory if it doesn't exist\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Bundle all artifacts needed for inference\n",
    "artifacts = {\n",
    "    # Trained model\n",
    "    \"model\": lgbm,\n",
    "    \n",
    "    # KMeans transformers for zone clustering\n",
    "    \"kmeans_start\": kmeans_start,\n",
    "    \"kmeans_end\": kmeans_end,\n",
    "    \n",
    "    # Target encoding for vehicle_id\n",
    "    \"vehicle_means\": vehicle_means,\n",
    "    \"global_mean\": float(global_mean),\n",
    "    \n",
    "    # Feature lists (CRITICAL: must match training order)\n",
    "    \"numeric_features\": numeric_features.copy(),\n",
    "    \"dummy_columns\": list(train_encoded.columns),\n",
    "    \n",
    "    # Distance bucket configuration\n",
    "    \"bucket_bins\": [0, 10, 50, 200, float('inf')],\n",
    "    \"bucket_labels\": ['0-10km', '10-50km', '50-200km', '200+km'],\n",
    "    \n",
    "    # Thresholds used in aggregation\n",
    "    \"stop_threshold_kmh\": 3.0,\n",
    "    \"overspeed_threshold_kmh\": 90.0,\n",
    "    \n",
    "    # Constants\n",
    "    \"epsilon\": 1e-6,\n",
    "    \n",
    "    # Metadata\n",
    "    \"model_version\": \"lgbm_refactored_v2\",\n",
    "    \"training_timestamp\": time.time(),\n",
    "    \"train_size\": len(train_data),\n",
    "    \"test_size\": len(test_data),\n",
    "    \"test_mae\": float(test_mae_lgbm),\n",
    "    \"test_rmse\": float(test_rmse_lgbm),\n",
    "    \"test_r2\": float(test_r2_lgbm),\n",
    "    \"feature_count\": X_train.shape[1],\n",
    "    \"best_iteration\": lgbm.best_iteration_\n",
    "}\n",
    "\n",
    "# Save artifacts\n",
    "artifacts_path = 'models/model_artifacts.joblib'\n",
    "joblib.dump(artifacts, artifacts_path)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MODEL ARTIFACTS SAVED\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Path: {artifacts_path}\")\n",
    "print(f\"Model version: {artifacts['model_version']}\")\n",
    "print(f\"Test MAE: {artifacts['test_mae']:.4f}\")\n",
    "print(f\"Test RMSE: {artifacts['test_rmse']:.4f}\")\n",
    "print(f\"Test R²: {artifacts['test_r2']:.4f}\")\n",
    "print(f\"Total features: {artifacts['feature_count']}\")\n",
    "print(f\"  Numeric: {len(artifacts['numeric_features'])}\")\n",
    "print(f\"  Categorical (one-hot): {len(artifacts['dummy_columns'])}\")\n",
    "print(f\"Unique vehicles: {len(artifacts['vehicle_means'])}\")\n",
    "print(f\"Best iteration: {artifacts['best_iteration']}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca22e1a0",
   "metadata": {},
   "source": [
    "## 17. API Input Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbd31b8",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"REQUIRED API INPUTS FOR INFERENCE (POST-TRIP SCORING)\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\"\"\n",
    "The Flask/FastAPI endpoint needs the following from the client:\n",
    "\n",
    "REQUIRED FIELDS:\n",
    "  - vehicle_id (string): Vehicle identifier\n",
    "  - timestamp_start (ISO datetime): Trip start time\n",
    "  - lat_start, lon_start (float): Origin coordinates\n",
    "  - lat_end, lon_end (float): Destination coordinates\n",
    "  - Trip_Distance_km (float): Total distance traveled\n",
    "  - Trip_Duration_min (float): Total duration in minutes\n",
    "  - Idle_Time_min (float): Idle time in minutes\n",
    "  - Moving_Time_min (float): Moving time in minutes\n",
    "  - Avg_Speed (float): Average speed in km/h\n",
    "  - Idle_Percentage (float): Percentage of time idle (0-100)\n",
    "  - Age_of_Truck_months (float): Age of vehicle in months\n",
    "\n",
    "OPTIONAL (for richer predictions with telemetry):\n",
    "  - If you can also provide row-level telemetry, compute aggregates:\n",
    "    - speed_mean, speed_median, speed_p25, speed_p75, speed_p90\n",
    "    - speed_std, speed_iqr, speed_cv\n",
    "    - stop_count, overspeed_count\n",
    "    - idle_event_count, moving_event_count\n",
    "    - Normalized rates: *_per_100km, *_per_hour\n",
    "\n",
    "RESPONSE:\n",
    "  - predicted_route_efficiency (float, 0-1)\n",
    "  - model_version (string)\n",
    "  - Optional: top_features (list of {\"feature\": str, \"contribution\": float})\n",
    "\"\"\"\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f16d63e",
   "metadata": {},
   "source": [
    "## 18. Test Inference Pipeline (Simulate API Call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f463e3bf",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Reload artifacts to verify they work\n",
    "loaded_artifacts = joblib.load('models/model_artifacts.joblib')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ARTIFACT LOADING VERIFICATION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"✓ Model type: {type(loaded_artifacts['model']).__name__}\")\n",
    "print(f\"✓ KMeans start clusters: {loaded_artifacts['kmeans_start'].n_clusters}\")\n",
    "print(f\"✓ KMeans end clusters: {loaded_artifacts['kmeans_end'].n_clusters}\")\n",
    "print(f\"✓ Vehicle means count: {len(loaded_artifacts['vehicle_means'])}\")\n",
    "print(f\"✓ Global mean: {loaded_artifacts['global_mean']:.4f}\")\n",
    "print(f\"✓ Numeric features: {len(loaded_artifacts['numeric_features'])}\")\n",
    "print(f\"✓ Dummy columns: {len(loaded_artifacts['dummy_columns'])}\")\n",
    "print(f\"✓ Model version: {loaded_artifacts['model_version']}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Test prediction on a single sample from test set\n",
    "sample_idx = 10\n",
    "sample_row = test_data.iloc[sample_idx]\n",
    "\n",
    "print(f\"\\nTEST PREDICTION ON SAMPLE (index {sample_idx}):\")\n",
    "print(f\"Trip ID: {sample_row['trip_id']}\")\n",
    "print(f\"Vehicle ID: {sample_row['vehicle_id']}\")\n",
    "print(f\"Actual route_efficiency: {sample_row['route_efficiency']:.4f}\")\n",
    "\n",
    "# Get the corresponding X_test row\n",
    "sample_X = X_test.iloc[sample_idx:sample_idx+1]\n",
    "sample_pred = loaded_artifacts['model'].predict(sample_X)[0]\n",
    "\n",
    "print(f\"Predicted route_efficiency: {sample_pred:.4f}\")\n",
    "print(f\"Absolute error: {abs(sample_pred - sample_row['route_efficiency']):.4f}\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n✓ All artifacts loaded successfully!\")\n",
    "print(\"✓ Model is ready for deployment in Flask/FastAPI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbfcc8b",
   "metadata": {},
   "source": [
    "# model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "224759ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.cluster import KMeans\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318c52cb",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "179f5123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trips shape: (145995, 22)\n",
      "\n",
      "Trips columns: ['vehicle_id', 'trip_id', 'timestamp_start', 'timestamp_end', 'lat_start', 'lon_start', 'lat_end', 'lon_end', 'Trip_Distance_km', 'Trip_Duration_min', 'Avg_Speed', 'Idle_Time_min', 'Moving_Time_min', 'Idle_Percentage', 'Distance_Efficiency', 'Time_Efficiency', 'Idle_Efficiency', 'Signal_Reliability', 'Age_of_Truck_months', 'total_rows', 'QA_negative_distance', 'QA_zero_duration']\n"
     ]
    }
   ],
   "source": [
    "# Load trips data (backbone) - TRIPS-ONLY PIPELINE\n",
    "trips = pd.read_csv('data/safetruck_data_iter1_trips.csv')\n",
    "\n",
    "print(f\"Trips shape: {trips.shape}\")\n",
    "print(\"\\nTrips columns:\", trips.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f786f3b",
   "metadata": {},
   "source": [
    "## 2. QA Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2b1ae5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial trips: 145995\n",
      "After QA filters: 55261\n",
      "Removed 90734 trips (62.1%)\n"
     ]
    }
   ],
   "source": [
    "# Convert timestamps to datetime\n",
    "trips['timestamp_start'] = pd.to_datetime(trips['timestamp_start'])\n",
    "trips['timestamp_end'] = pd.to_datetime(trips['timestamp_end'])\n",
    "\n",
    "# Apply strict QA filters\n",
    "print(f\"Initial trips: {len(trips)}\")\n",
    "\n",
    "# Drop QA anomalies\n",
    "trips_clean = trips[\n",
    "    (trips['QA_zero_duration'] == False) &\n",
    "    (trips['QA_negative_distance'] == False) &\n",
    "    (trips['Trip_Duration_min'] > 0) &\n",
    "    (trips['Trip_Distance_km'] > 0) &\n",
    "    (trips['lat_start'].notna()) &\n",
    "    (trips['lon_start'].notna()) &\n",
    "    (trips['lat_end'].notna()) &\n",
    "    (trips['lon_end'].notna())\n",
    "].copy()\n",
    "\n",
    "print(f\"After QA filters: {len(trips_clean)}\")\n",
    "print(f\"Removed {len(trips) - len(trips_clean)} trips ({100*(len(trips) - len(trips_clean))/len(trips):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f67c35",
   "metadata": {},
   "source": [
    "## 3. Geographic Features & Haversine Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3664afdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geographic features computed:\n",
      "       haversine_km  detour_ratio  directness_efficiency       bearing\n",
      "count  55261.000000  5.526100e+04           55261.000000  55261.000000\n",
      "mean     194.512867  2.470591e+04               0.225262     28.589770\n",
      "std      393.672828  3.218665e+06               0.397734    103.461589\n",
      "min        0.000000  8.680198e-07               0.000000   -179.977752\n",
      "25%        2.415024  7.530712e+00               0.000329    -51.192373\n",
      "50%      128.358150  8.945813e+02               0.001118     -0.022469\n",
      "75%      300.611110  3.000000e+03               0.132790    140.479564\n",
      "max    11555.312191  7.521470e+08               1.000000    180.000000\n"
     ]
    }
   ],
   "source": [
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Calculate great-circle distance in km using haversine formula\"\"\"\n",
    "    R = 6371  # Earth radius in km\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    return R * c\n",
    "\n",
    "def calculate_bearing(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Calculate bearing from start to end point in degrees\"\"\"\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlon = lon2 - lon1\n",
    "    x = np.sin(dlon) * np.cos(lat2)\n",
    "    y = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(dlon)\n",
    "    bearing = np.arctan2(x, y)\n",
    "    return np.degrees(bearing)\n",
    "\n",
    "# Calculate haversine distance\n",
    "trips_clean['haversine_km'] = haversine(\n",
    "    trips_clean['lat_start'], \n",
    "    trips_clean['lon_start'],\n",
    "    trips_clean['lat_end'], \n",
    "    trips_clean['lon_end']\n",
    ")\n",
    "\n",
    "# Calculate detour ratio\n",
    "epsilon = 1e-6\n",
    "trips_clean['detour_ratio'] = trips_clean['Trip_Distance_km'] / (trips_clean['haversine_km'] + epsilon)\n",
    "\n",
    "# Calculate directness efficiency (clipped to [0, 1])\n",
    "trips_clean['directness_efficiency'] = np.clip(\n",
    "    trips_clean['haversine_km'] / (trips_clean['Trip_Distance_km'] + epsilon), \n",
    "    0, 1\n",
    ")\n",
    "\n",
    "# Calculate bearing\n",
    "trips_clean['bearing'] = calculate_bearing(\n",
    "    trips_clean['lat_start'], \n",
    "    trips_clean['lon_start'],\n",
    "    trips_clean['lat_end'], \n",
    "    trips_clean['lon_end']\n",
    ")\n",
    "\n",
    "# Delta lat/lon\n",
    "trips_clean['delta_lat'] = trips_clean['lat_end'] - trips_clean['lat_start']\n",
    "trips_clean['delta_lon'] = trips_clean['lon_end'] - trips_clean['lon_start']\n",
    "\n",
    "print(\"Geographic features computed:\")\n",
    "print(trips_clean[['haversine_km', 'detour_ratio', 'directness_efficiency', 'bearing']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79ad3dc",
   "metadata": {},
   "source": [
    "## 4. Time Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61adcc1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time features computed:\n",
      "   hour_of_day  day_of_week  hour_sin  hour_cos   dow_sin   dow_cos\n",
      "0            8            4  0.866025 -0.500000 -0.433884 -0.900969\n",
      "1           11            4  0.258819 -0.965926 -0.433884 -0.900969\n",
      "2           15            4 -0.707107 -0.707107 -0.433884 -0.900969\n",
      "3           15            4 -0.707107 -0.707107 -0.433884 -0.900969\n",
      "4           16            4 -0.866025 -0.500000 -0.433884 -0.900969\n"
     ]
    }
   ],
   "source": [
    "# Extract time features\n",
    "trips_clean['hour_of_day'] = trips_clean['timestamp_start'].dt.hour\n",
    "trips_clean['day_of_week'] = trips_clean['timestamp_start'].dt.dayofweek\n",
    "\n",
    "# Cyclical encoding for hour (0-23)\n",
    "trips_clean['hour_sin'] = np.sin(2 * np.pi * trips_clean['hour_of_day'] / 24)\n",
    "trips_clean['hour_cos'] = np.cos(2 * np.pi * trips_clean['hour_of_day'] / 24)\n",
    "\n",
    "# Cyclical encoding for day of week (0-6)\n",
    "trips_clean['dow_sin'] = np.sin(2 * np.pi * trips_clean['day_of_week'] / 7)\n",
    "trips_clean['dow_cos'] = np.cos(2 * np.pi * trips_clean['day_of_week'] / 7)\n",
    "\n",
    "print(\"Time features computed:\")\n",
    "print(trips_clean[['hour_of_day', 'day_of_week', 'hour_sin', 'hour_cos', 'dow_sin', 'dow_cos']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221138b0",
   "metadata": {},
   "source": [
    "## 5. Operational Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0cee91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operational features computed:\n",
      "         idle_ratio  moving_ratio   idle_per_km\n",
      "count  55261.000000  55261.000000  5.526100e+04\n",
      "mean       0.422166      0.577759  5.633546e+00\n",
      "std        0.363724      0.363754  9.136388e+01\n",
      "min        0.000000      0.000000  0.000000e+00\n",
      "25%        0.000000      0.285712  0.000000e+00\n",
      "50%        0.370370      0.629628  2.196183e-07\n",
      "75%        0.714284      0.999940  1.410396e-06\n",
      "max        1.000000      1.000000  8.173638e+03\n",
      "\n",
      "Distance bucket distribution:\n",
      "distance_bucket\n",
      "0-10km      13361\n",
      "10-50km       448\n",
      "50-200km      449\n",
      "200+km      41003\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Calculate operational ratios\n",
    "trips_clean['idle_ratio'] = trips_clean['Idle_Time_min'] / (trips_clean['Trip_Duration_min'] + epsilon)\n",
    "trips_clean['moving_ratio'] = trips_clean['Moving_Time_min'] / (trips_clean['Trip_Duration_min'] + epsilon)\n",
    "trips_clean['idle_per_km'] = trips_clean['Idle_Time_min'] / (trips_clean['Trip_Distance_km'] + epsilon)\n",
    "\n",
    "# Distance buckets for speed normalization\n",
    "trips_clean['distance_bucket'] = pd.cut(\n",
    "    trips_clean['Trip_Distance_km'],\n",
    "    bins=[0, 10, 50, 200, float('inf')],\n",
    "    labels=['0-10km', '10-50km', '50-200km', '200+km']\n",
    ")\n",
    "\n",
    "print(\"Operational features computed:\")\n",
    "print(trips_clean[['idle_ratio', 'moving_ratio', 'idle_per_km', 'distance_bucket']].describe())\n",
    "print(\"\\nDistance bucket distribution:\")\n",
    "print(trips_clean['distance_bucket'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ebcd84",
   "metadata": {},
   "source": [
    "## 6. Compute Target Variable: route_efficiency (Trips-Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8176ac16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using trips-only pipeline - no external joins needed\n"
     ]
    }
   ],
   "source": [
    "# TRIPS-ONLY: Skip row-level and combined data joins\n",
    "# All features will be derived directly from trips CSV\n",
    "print(\"Using trips-only pipeline - no external joins needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defa25eb",
   "metadata": {},
   "source": [
    "## 7. Compute Target Variable: route_efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed56de0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 44208 trips (from 2025-08-01 00:00:17+00:00 to 2025-08-25 20:13:56+00:00)\n",
      "Test set: 11053 trips (from 2025-08-25 20:14:04+00:00 to 2025-08-31 13:44:31+00:00)\n",
      "\n",
      "Reference speeds per bucket (from training data):\n",
      "  0-10km: 42.16 km/h\n",
      "  10-50km: 22.03 km/h\n",
      "  50-200km: 30.46 km/h\n",
      "  200+km: 113646288.75 km/h\n",
      "\n",
      "Target variable (route_efficiency) distribution:\n",
      "count    44208.000000\n",
      "mean         0.425302\n",
      "std          0.269620\n",
      "min          0.000051\n",
      "25%          0.201379\n",
      "50%          0.350117\n",
      "75%          0.500452\n",
      "max          1.000000\n",
      "Name: route_efficiency, dtype: float64\n",
      "\n",
      "Correlation with provided efficiencies (train):\n",
      "route_efficiency       1.000000\n",
      "Distance_Efficiency    0.872618\n",
      "Time_Efficiency       -0.666498\n",
      "Idle_Efficiency        0.638403\n",
      "Name: route_efficiency, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Time-based split first (to compute ref speeds on training data only)\n",
    "trips_clean = trips_clean.sort_values('timestamp_start').reset_index(drop=True)\n",
    "\n",
    "# 80-20 time split\n",
    "split_idx = int(len(trips_clean) * 0.8)\n",
    "train_data = trips_clean.iloc[:split_idx].copy()\n",
    "test_data = trips_clean.iloc[split_idx:].copy()\n",
    "\n",
    "print(f\"Train set: {len(train_data)} trips (from {train_data['timestamp_start'].min()} to {train_data['timestamp_start'].max()})\")\n",
    "print(f\"Test set: {len(test_data)} trips (from {test_data['timestamp_start'].min()} to {test_data['timestamp_start'].max()})\")\n",
    "\n",
    "# Compute reference speeds per distance bucket on TRAIN data only\n",
    "ref_speeds = train_data.groupby('distance_bucket')['Avg_Speed'].median().to_dict()\n",
    "print(\"\\nReference speeds per bucket (from training data):\")\n",
    "for bucket, speed in ref_speeds.items():\n",
    "    print(f\"  {bucket}: {speed:.2f} km/h\")\n",
    "\n",
    "# Compute speed efficiency for both train and test\n",
    "def compute_speed_efficiency(row):\n",
    "    bucket = row['distance_bucket']\n",
    "    ref_speed = ref_speeds.get(bucket, row['Avg_Speed'])  # fallback to own speed if bucket not in ref\n",
    "    return np.clip(row['Avg_Speed'] / (ref_speed + epsilon), 0, 1)\n",
    "\n",
    "train_data['speed_efficiency'] = train_data.apply(compute_speed_efficiency, axis=1)\n",
    "test_data['speed_efficiency'] = test_data.apply(compute_speed_efficiency, axis=1)\n",
    "\n",
    "# Idle efficiency\n",
    "train_data['idle_efficiency'] = 1 - train_data['idle_ratio']\n",
    "test_data['idle_efficiency'] = 1 - test_data['idle_ratio']\n",
    "\n",
    "# Compute composite route_efficiency = 0.5*directness + 0.3*idle + 0.2*speed\n",
    "train_data['route_efficiency'] = (\n",
    "    0.5 * train_data['directness_efficiency'] +\n",
    "    0.3 * train_data['idle_efficiency'] +\n",
    "    0.2 * train_data['speed_efficiency']\n",
    ")\n",
    "\n",
    "test_data['route_efficiency'] = (\n",
    "    0.5 * test_data['directness_efficiency'] +\n",
    "    0.3 * test_data['idle_efficiency'] +\n",
    "    0.2 * test_data['speed_efficiency']\n",
    ")\n",
    "\n",
    "print(\"\\nTarget variable (route_efficiency) distribution:\")\n",
    "print(train_data['route_efficiency'].describe())\n",
    "print(f\"\\nCorrelation with provided efficiencies (train):\")\n",
    "print(train_data[['route_efficiency', 'Distance_Efficiency', 'Time_Efficiency', 'Idle_Efficiency']].corr()['route_efficiency'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1868a3b8",
   "metadata": {},
   "source": [
    "## 8. Zone Clustering (k=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40fe0383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 50 start zones and 50 end zones\n",
      "Train zone_start_id range: 0-49\n",
      "Train zone_end_id range: 0-49\n"
     ]
    }
   ],
   "source": [
    "# Fit k-means on training data for start and end zones\n",
    "k = 50\n",
    "\n",
    "# Start zones\n",
    "kmeans_start = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "train_data['zone_start_id'] = kmeans_start.fit_predict(train_data[['lat_start', 'lon_start']])\n",
    "test_data['zone_start_id'] = kmeans_start.predict(test_data[['lat_start', 'lon_start']])\n",
    "\n",
    "# End zones\n",
    "kmeans_end = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "train_data['zone_end_id'] = kmeans_end.fit_predict(train_data[['lat_end', 'lon_end']])\n",
    "test_data['zone_end_id'] = kmeans_end.predict(test_data[['lat_end', 'lon_end']])\n",
    "\n",
    "print(f\"Created {k} start zones and {k} end zones\")\n",
    "print(f\"Train zone_start_id range: {train_data['zone_start_id'].min()}-{train_data['zone_start_id'].max()}\")\n",
    "print(f\"Train zone_end_id range: {train_data['zone_end_id'].min()}-{train_data['zone_end_id'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9103f50c",
   "metadata": {},
   "source": [
    "## 9. Prepare Feature Sets (Trips-Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80b399fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical features (23): ['Trip_Distance_km', 'Trip_Duration_min', 'Avg_Speed', 'Idle_Time_min', 'Moving_Time_min', 'Idle_Percentage', 'haversine_km', 'detour_ratio', 'bearing', 'delta_lat', 'delta_lon', 'lat_start', 'lon_start', 'lat_end', 'lon_end', 'hour_sin', 'hour_cos', 'dow_sin', 'dow_cos', 'idle_ratio', 'moving_ratio', 'idle_per_km', 'Age_of_Truck_months']\n",
      "\n",
      "Categorical features (4): ['vehicle_id', 'zone_start_id', 'zone_end_id', 'distance_bucket']\n"
     ]
    }
   ],
   "source": [
    "# Define numerical features (trips-only, no row/combined aggregates)\n",
    "numeric_features = [\n",
    "    # Base trip metrics\n",
    "    'Trip_Distance_km', 'Trip_Duration_min', 'Avg_Speed',\n",
    "    'Idle_Time_min', 'Moving_Time_min', 'Idle_Percentage',\n",
    "    # Geographic\n",
    "    'haversine_km', 'detour_ratio', 'bearing', 'delta_lat', 'delta_lon',\n",
    "    'lat_start', 'lon_start', 'lat_end', 'lon_end',\n",
    "    # Time (cyclical)\n",
    "    'hour_sin', 'hour_cos', 'dow_sin', 'dow_cos',\n",
    "    # Operational\n",
    "    'idle_ratio', 'moving_ratio', 'idle_per_km',\n",
    "    # Metadata\n",
    "    'Age_of_Truck_months'\n",
    "]\n",
    "\n",
    "# Categorical features\n",
    "categorical_features = ['vehicle_id', 'zone_start_id', 'zone_end_id', 'distance_bucket']\n",
    "\n",
    "# Filter to only features that exist in both train and test\n",
    "numeric_features = [f for f in numeric_features if f in train_data.columns and f in test_data.columns]\n",
    "categorical_features = [f for f in categorical_features if f in train_data.columns and f in test_data.columns]\n",
    "\n",
    "print(f\"Numerical features ({len(numeric_features)}): {numeric_features}\")\n",
    "print(f\"\\nCategorical features ({len(categorical_features)}): {categorical_features}\")\n",
    "\n",
    "# Fill NaN values in numeric features with 0 (should be minimal in trips-only)\n",
    "for feat in numeric_features:\n",
    "    train_data[feat] = train_data[feat].fillna(0)\n",
    "    test_data[feat] = test_data[feat].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015df2a1",
   "metadata": {},
   "source": [
    "## 11. Target Encoding for vehicle_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0e10f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target encoded vehicle_id using 31 unique vehicles\n",
      "Global mean route_efficiency: 0.4253\n"
     ]
    }
   ],
   "source": [
    "# Target encode vehicle_id using training data\n",
    "vehicle_means = train_data.groupby('vehicle_id')['route_efficiency'].mean()\n",
    "global_mean = train_data['route_efficiency'].mean()\n",
    "\n",
    "# Apply to train and test\n",
    "train_data['vehicle_id_encoded'] = train_data['vehicle_id'].map(vehicle_means).fillna(global_mean)\n",
    "test_data['vehicle_id_encoded'] = test_data['vehicle_id'].map(vehicle_means).fillna(global_mean)\n",
    "\n",
    "print(f\"Target encoded vehicle_id using {len(vehicle_means)} unique vehicles\")\n",
    "print(f\"Global mean route_efficiency: {global_mean:.4f}\")\n",
    "\n",
    "# Update feature lists\n",
    "numeric_features.append('vehicle_id_encoded')\n",
    "categorical_features.remove('vehicle_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d9d10b",
   "metadata": {},
   "source": [
    "## 12. Prepare X and y for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2bec71a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (44208, 128)\n",
      "X_test shape: (11053, 128)\n",
      "y_train shape: (44208,)\n",
      "y_test shape: (11053,)\n"
     ]
    }
   ],
   "source": [
    "# One-hot encode categorical features (zones and distance_bucket)\n",
    "# Use columns= to force encoding of all categorical features (even if they're integers)\n",
    "train_encoded = pd.get_dummies(train_data, columns=categorical_features, prefix=categorical_features)\n",
    "test_encoded = pd.get_dummies(test_data, columns=categorical_features, prefix=categorical_features)\n",
    "\n",
    "# Keep only the newly created dummy columns (remove original data columns)\n",
    "dummy_cols_train = [col for col in train_encoded.columns if any(col.startswith(f\"{cat}_\") for cat in categorical_features)]\n",
    "dummy_cols_test = [col for col in test_encoded.columns if any(col.startswith(f\"{cat}_\") for cat in categorical_features)]\n",
    "\n",
    "train_encoded = train_encoded[dummy_cols_train]\n",
    "test_encoded = test_encoded[dummy_cols_test]\n",
    "\n",
    "# Align columns between train and test (test must have all train columns)\n",
    "test_encoded = test_encoded.reindex(columns=train_encoded.columns, fill_value=0)\n",
    "\n",
    "# Combine numeric and categorical\n",
    "X_train = pd.concat([\n",
    "    train_data[numeric_features].reset_index(drop=True),\n",
    "    train_encoded.reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "X_test = pd.concat([\n",
    "    test_data[numeric_features].reset_index(drop=True),\n",
    "    test_encoded.reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "y_train = train_data['route_efficiency'].values\n",
    "y_test = test_data['route_efficiency'].values\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61775560",
   "metadata": {},
   "source": [
    "## 13. Baseline Model: Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb2a47ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "RIDGE REGRESSION RESULTS\n",
      "============================================================\n",
      "Train - MAE: 0.0626, RMSE: 0.0895, R²: 0.8898\n",
      "Test  - MAE: 0.0654, RMSE: 0.0933, R²: 0.8821\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Scale features for linear model\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train Ridge regression\n",
    "ridge = Ridge(alpha=1.0, random_state=42)\n",
    "ridge.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_ridge = ridge.predict(X_train_scaled)\n",
    "y_test_pred_ridge = ridge.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate\n",
    "train_mae_ridge = mean_absolute_error(y_train, y_train_pred_ridge)\n",
    "train_rmse_ridge = np.sqrt(mean_squared_error(y_train, y_train_pred_ridge))\n",
    "train_r2_ridge = r2_score(y_train, y_train_pred_ridge)\n",
    "\n",
    "test_mae_ridge = mean_absolute_error(y_test, y_test_pred_ridge)\n",
    "test_rmse_ridge = np.sqrt(mean_squared_error(y_test, y_test_pred_ridge))\n",
    "test_r2_ridge = r2_score(y_test, y_test_pred_ridge)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"RIDGE REGRESSION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Train - MAE: {train_mae_ridge:.4f}, RMSE: {train_rmse_ridge:.4f}, R²: {train_r2_ridge:.4f}\")\n",
    "print(f\"Test  - MAE: {test_mae_ridge:.4f}, RMSE: {test_rmse_ridge:.4f}, R²: {test_r2_ridge:.4f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616d1275",
   "metadata": {},
   "source": [
    "## 14. Main Model: LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c86e8bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LIGHTGBM RESULTS\n",
      "============================================================\n",
      "Train - MAE: 0.0023, RMSE: 0.0044, R²: 0.9997\n",
      "Test  - MAE: 0.0026, RMSE: 0.0054, R²: 0.9996\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Train LightGBM (no scaling needed for tree models)\n",
    "lgbm = lgb.LGBMRegressor(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=7,\n",
    "    num_leaves=31,\n",
    "    min_child_samples=20,\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "lgbm.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    eval_metric='mae',\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=20, verbose=False)]\n",
    ")\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_lgbm = lgbm.predict(X_train)\n",
    "y_test_pred_lgbm = lgbm.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "train_mae_lgbm = mean_absolute_error(y_train, y_train_pred_lgbm)\n",
    "train_rmse_lgbm = np.sqrt(mean_squared_error(y_train, y_train_pred_lgbm))\n",
    "train_r2_lgbm = r2_score(y_train, y_train_pred_lgbm)\n",
    "\n",
    "test_mae_lgbm = mean_absolute_error(y_test, y_test_pred_lgbm)\n",
    "test_rmse_lgbm = np.sqrt(mean_squared_error(y_test, y_test_pred_lgbm))\n",
    "test_r2_lgbm = r2_score(y_test, y_test_pred_lgbm)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LIGHTGBM RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Train - MAE: {train_mae_lgbm:.4f}, RMSE: {train_rmse_lgbm:.4f}, R²: {train_r2_lgbm:.4f}\")\n",
    "print(f\"Test  - MAE: {test_mae_lgbm:.4f}, RMSE: {test_rmse_lgbm:.4f}, R²: {test_r2_lgbm:.4f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5176fb3e",
   "metadata": {},
   "source": [
    "## 15. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e784a767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 20 Most Important Features:\n",
      "                 feature  importance\n",
      "               Avg_Speed        1299\n",
      "            detour_ratio        1129\n",
      "             idle_per_km         946\n",
      "         Idle_Percentage         467\n",
      "              idle_ratio         430\n",
      "            moving_ratio         258\n",
      "            haversine_km         176\n",
      "       Trip_Duration_min         168\n",
      "  distance_bucket_200+km         156\n",
      "        Trip_Distance_km         142\n",
      "           Idle_Time_min         132\n",
      "         Moving_Time_min         119\n",
      " distance_bucket_10-50km         118\n",
      "distance_bucket_50-200km          81\n",
      "      vehicle_id_encoded          48\n",
      "               lon_start          36\n",
      "               lat_start          30\n",
      "               delta_lat          29\n",
      "     Age_of_Truck_months          29\n",
      "                hour_cos          26\n"
     ]
    }
   ],
   "source": [
    "# Get feature importances from LightGBM\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': lgbm.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 20 Most Important Features:\")\n",
    "print(feature_importance.head(20).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65eea790",
   "metadata": {},
   "source": [
    "## 16. Model Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99872cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODEL COMPARISON SUMMARY\n",
      "================================================================================\n",
      "           Model  Train MAE  Test MAE  Train RMSE  Test RMSE  Train R²  Test R²\n",
      "Ridge Regression   0.062578  0.065404    0.089511   0.093338  0.889779 0.882053\n",
      "        LightGBM   0.002335  0.002622    0.004380   0.005449  0.999736 0.999598\n",
      "================================================================================\n",
      "\n",
      "Best model by Test MAE: LightGBM\n",
      "Best model by Test R²: LightGBM\n"
     ]
    }
   ],
   "source": [
    "# Summary comparison\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['Ridge Regression', 'LightGBM'],\n",
    "    'Train MAE': [train_mae_ridge, train_mae_lgbm],\n",
    "    'Test MAE': [test_mae_ridge, test_mae_lgbm],\n",
    "    'Train RMSE': [train_rmse_ridge, train_rmse_lgbm],\n",
    "    'Test RMSE': [test_rmse_ridge, test_rmse_lgbm],\n",
    "    'Train R²': [train_r2_ridge, train_r2_lgbm],\n",
    "    'Test R²': [test_r2_ridge, test_r2_lgbm]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(results.to_string(index=False))\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nBest model by Test MAE: {results.loc[results['Test MAE'].idxmin(), 'Model']}\")\n",
    "print(f\"Best model by Test R²: {results.loc[results['Test R²'].idxmax(), 'Model']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e91086",
   "metadata": {},
   "source": [
    "## 17. Package & Pickle Model Artifacts for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29d8ecea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MODEL ARTIFACTS SAVED\n",
      "================================================================================\n",
      "Path: models/model_artifacts.joblib\n",
      "Model version: lgbm_trips_only_v1\n",
      "Test MAE: 0.0026\n",
      "Test R²: 0.9996\n",
      "Total features: 128\n",
      "Numeric features: 24\n",
      "Dummy columns: 104\n",
      "Unique vehicles: 31\n",
      "================================================================================\n",
      "\n",
      "REQUIRED API INPUTS (for inference):\n",
      "  - vehicle_id\n",
      "  - timestamp_start\n",
      "  - lat_start\n",
      "  - lon_start\n",
      "  - lat_end\n",
      "  - lon_end\n",
      "  - Trip_Distance_km\n",
      "  - Trip_Duration_min\n",
      "  - Idle_Time_min\n",
      "  - Moving_Time_min\n",
      "  - Avg_Speed\n",
      "  - Idle_Percentage\n",
      "  - Age_of_Truck_months\n",
      "\n",
      "NOTE: All engineered features (geo, time, operational, zones) will be\n",
      "      computed from these raw inputs at inference time using the saved\n",
      "      transformers and the same feature engineering logic.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Bundle all artifacts needed for inference\n",
    "artifacts = {\n",
    "    # Trained model\n",
    "    \"model\": lgbm,\n",
    "    \n",
    "    # KMeans transformers for zone clustering\n",
    "    \"kmeans_start\": kmeans_start,\n",
    "    \"kmeans_end\": kmeans_end,\n",
    "    \n",
    "    # Target encoding for vehicle_id\n",
    "    \"vehicle_means\": vehicle_means,\n",
    "    \"global_mean\": float(global_mean),\n",
    "    \n",
    "    # Reference speeds per distance bucket (for versioning/audit, not needed in prediction)\n",
    "    \"ref_speeds\": ref_speeds,\n",
    "    \n",
    "    # Feature lists (CRITICAL: must match training order)\n",
    "    \"numeric_features\": numeric_features.copy(),\n",
    "    \"dummy_columns\": list(train_encoded.columns),  # exact one-hot columns from training\n",
    "    \n",
    "    # Distance bucket configuration\n",
    "    \"bucket_bins\": [0, 10, 50, 200, float('inf')],\n",
    "    \"bucket_labels\": ['0-10km', '10-50km', '50-200km', '200+km'],\n",
    "    \n",
    "    # Constants\n",
    "    \"epsilon\": float(epsilon),\n",
    "    \n",
    "    # Metadata\n",
    "    \"model_version\": \"lgbm_trips_only_v1\",\n",
    "    \"training_timestamp\": time.time(),\n",
    "    \"train_size\": len(train_data),\n",
    "    \"test_size\": len(test_data),\n",
    "    \"test_mae\": float(test_mae_lgbm),\n",
    "    \"test_r2\": float(test_r2_lgbm),\n",
    "    \"feature_count\": X_train.shape[1]\n",
    "}\n",
    "\n",
    "# Save artifacts\n",
    "artifacts_path = 'models/model_artifacts.joblib'\n",
    "joblib.dump(artifacts, artifacts_path)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MODEL ARTIFACTS SAVED\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Path: {artifacts_path}\")\n",
    "print(f\"Model version: {artifacts['model_version']}\")\n",
    "print(f\"Test MAE: {artifacts['test_mae']:.4f}\")\n",
    "print(f\"Test R²: {artifacts['test_r2']:.4f}\")\n",
    "print(f\"Total features: {artifacts['feature_count']}\")\n",
    "print(f\"Numeric features: {len(artifacts['numeric_features'])}\")\n",
    "print(f\"Dummy columns: {len(artifacts['dummy_columns'])}\")\n",
    "print(f\"Unique vehicles: {len(artifacts['vehicle_means'])}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Show what's required from API payload\n",
    "print(\"\\nREQUIRED API INPUTS (for inference):\")\n",
    "required_inputs = [\n",
    "    \"vehicle_id\",\n",
    "    \"timestamp_start\",\n",
    "    \"lat_start\", \"lon_start\", \n",
    "    \"lat_end\", \"lon_end\",\n",
    "    \"Trip_Distance_km\",\n",
    "    \"Trip_Duration_min\",\n",
    "    \"Idle_Time_min\",\n",
    "    \"Moving_Time_min\",\n",
    "    \"Avg_Speed\",\n",
    "    \"Idle_Percentage\",\n",
    "    \"Age_of_Truck_months\"\n",
    "]\n",
    "for inp in required_inputs:\n",
    "    print(f\"  - {inp}\")\n",
    "    \n",
    "print(\"\\nNOTE: All engineered features (geo, time, operational, zones) will be\")\n",
    "print(\"      computed from these raw inputs at inference time using the saved\")\n",
    "print(\"      transformers and the same feature engineering logic.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a52a618",
   "metadata": {},
   "source": [
    "## 18. Verify Artifact Loading (Sanity Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "884c30bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ARTIFACT LOADING VERIFICATION\n",
      "================================================================================\n",
      "✓ Model type: LGBMRegressor\n",
      "✓ KMeans start clusters: 50\n",
      "✓ KMeans end clusters: 50\n",
      "✓ Vehicle means count: 31\n",
      "✓ Global mean: 0.4253\n",
      "✓ Numeric features: 24\n",
      "✓ Dummy columns: 104\n",
      "✓ Distance buckets: ['0-10km', '10-50km', '50-200km', '200+km']\n",
      "✓ Model version: lgbm_trips_only_v1\n",
      "✓ Training timestamp: 2025-10-24 02:52:49\n",
      "================================================================================\n",
      "\n",
      "TEST PREDICTION ON SAMPLE:\n",
      "Sample trip_id: 118523\n",
      "Actual route_efficiency: 0.4279\n",
      "Predicted route_efficiency: 0.4328\n",
      "Absolute error: 0.0050\n",
      "================================================================================\n",
      "\n",
      "✓ All artifacts loaded successfully!\n",
      "✓ Model is ready for deployment in Flask API\n"
     ]
    }
   ],
   "source": [
    "# Reload artifacts to verify they can be loaded correctly\n",
    "loaded_artifacts = joblib.load('models/model_artifacts.joblib')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ARTIFACT LOADING VERIFICATION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"✓ Model type: {type(loaded_artifacts['model']).__name__}\")\n",
    "print(f\"✓ KMeans start clusters: {loaded_artifacts['kmeans_start'].n_clusters}\")\n",
    "print(f\"✓ KMeans end clusters: {loaded_artifacts['kmeans_end'].n_clusters}\")\n",
    "print(f\"✓ Vehicle means count: {len(loaded_artifacts['vehicle_means'])}\")\n",
    "print(f\"✓ Global mean: {loaded_artifacts['global_mean']:.4f}\")\n",
    "print(f\"✓ Numeric features: {len(loaded_artifacts['numeric_features'])}\")\n",
    "print(f\"✓ Dummy columns: {len(loaded_artifacts['dummy_columns'])}\")\n",
    "print(f\"✓ Distance buckets: {loaded_artifacts['bucket_labels']}\")\n",
    "print(f\"✓ Model version: {loaded_artifacts['model_version']}\")\n",
    "print(f\"✓ Training timestamp: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(loaded_artifacts['training_timestamp']))}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Test prediction on a single sample from test set\n",
    "sample_idx = 0\n",
    "sample_row = test_data.iloc[sample_idx]\n",
    "\n",
    "print(\"\\nTEST PREDICTION ON SAMPLE:\")\n",
    "print(f\"Sample trip_id: {sample_row.get('trip_id', 'N/A')}\")\n",
    "print(f\"Actual route_efficiency: {sample_row['route_efficiency']:.4f}\")\n",
    "\n",
    "# Get the corresponding X_test row\n",
    "sample_X = X_test.iloc[sample_idx:sample_idx+1]\n",
    "sample_pred = loaded_artifacts['model'].predict(sample_X)[0]\n",
    "\n",
    "print(f\"Predicted route_efficiency: {sample_pred:.4f}\")\n",
    "print(f\"Absolute error: {abs(sample_pred - sample_row['route_efficiency']):.4f}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n✓ All artifacts loaded successfully!\")\n",
    "print(\"✓ Model is ready for deployment in Flask API\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2257d0bb",
   "metadata": {},
   "source": [
    "## 19. Inference Example (How Flask API Will Use Artifacts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "056cea20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "INFERENCE FUNCTION TEST\n",
      "================================================================================\n",
      "Sample Index: 5\n",
      "Actual route_efficiency: 0.2001\n",
      "Predicted (via inference function): 0.2003\n",
      "Absolute error: 0.0003\n",
      "================================================================================\n",
      "\n",
      "✓ Inference function works correctly!\n",
      "✓ Ready to integrate into Flask API\n"
     ]
    }
   ],
   "source": [
    "def predict_route_efficiency(payload, artifacts):\n",
    "    \"\"\"\n",
    "    Reproduce the exact feature engineering pipeline for a single trip.\n",
    "    This is the core logic your Flask API will use.\n",
    "    \n",
    "    Args:\n",
    "        payload: dict with raw trip inputs\n",
    "        artifacts: dict loaded from model_artifacts.joblib\n",
    "    \n",
    "    Returns:\n",
    "        float: predicted route_efficiency [0, 1]\n",
    "    \"\"\"\n",
    "    from datetime import datetime\n",
    "    \n",
    "    # Extract artifacts\n",
    "    model = artifacts['model']\n",
    "    kmeans_start = artifacts['kmeans_start']\n",
    "    kmeans_end = artifacts['kmeans_end']\n",
    "    vehicle_means = artifacts['vehicle_means']\n",
    "    global_mean = artifacts['global_mean']\n",
    "    numeric_features = artifacts['numeric_features']\n",
    "    dummy_columns = artifacts['dummy_columns']\n",
    "    bucket_bins = artifacts['bucket_bins']\n",
    "    bucket_labels = artifacts['bucket_labels']\n",
    "    eps = artifacts['epsilon']\n",
    "    \n",
    "    # 1. Parse timestamp\n",
    "    ts = pd.to_datetime(payload['timestamp_start'])\n",
    "    hour_of_day = ts.hour\n",
    "    day_of_week = ts.dayofweek\n",
    "    \n",
    "    # 2. Geographic features\n",
    "    lat1, lon1 = payload['lat_start'], payload['lon_start']\n",
    "    lat2, lon2 = payload['lat_end'], payload['lon_end']\n",
    "    \n",
    "    # Haversine\n",
    "    R = 6371\n",
    "    lat1_r, lon1_r, lat2_r, lon2_r = np.radians([lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2_r - lat1_r\n",
    "    dlon = lon2_r - lon1_r\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1_r) * np.cos(lat2_r) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    haversine_km = R * c\n",
    "    \n",
    "    trip_dist = payload['Trip_Distance_km']\n",
    "    detour_ratio = trip_dist / (haversine_km + eps)\n",
    "    directness_eff = np.clip(haversine_km / (trip_dist + eps), 0, 1)\n",
    "    \n",
    "    # Bearing\n",
    "    x = np.sin(dlon) * np.cos(lat2_r)\n",
    "    y = np.cos(lat1_r) * np.sin(lat2_r) - np.sin(lat1_r) * np.cos(lat2_r) * np.cos(dlon)\n",
    "    bearing = np.degrees(np.arctan2(x, y))\n",
    "    \n",
    "    delta_lat = lat2 - lat1\n",
    "    delta_lon = lon2 - lon1\n",
    "    \n",
    "    # 3. Time features (cyclical)\n",
    "    hour_sin = np.sin(2 * np.pi * hour_of_day / 24)\n",
    "    hour_cos = np.cos(2 * np.pi * hour_of_day / 24)\n",
    "    dow_sin = np.sin(2 * np.pi * day_of_week / 7)\n",
    "    dow_cos = np.cos(2 * np.pi * day_of_week / 7)\n",
    "    \n",
    "    # 4. Operational features\n",
    "    trip_dur = payload['Trip_Duration_min']\n",
    "    idle_time = payload['Idle_Time_min']\n",
    "    moving_time = payload['Moving_Time_min']\n",
    "    \n",
    "    idle_ratio = idle_time / (trip_dur + eps)\n",
    "    moving_ratio = moving_time / (trip_dur + eps)\n",
    "    idle_per_km = idle_time / (trip_dist + eps)\n",
    "    \n",
    "    # Distance bucket\n",
    "    distance_bucket = pd.cut([trip_dist], bins=bucket_bins, labels=bucket_labels)[0]\n",
    "    \n",
    "    # 5. Vehicle encoding\n",
    "    vehicle_id = payload['vehicle_id']\n",
    "    vehicle_id_encoded = vehicle_means.get(vehicle_id, global_mean)\n",
    "    \n",
    "    # 6. Zone clustering\n",
    "    zone_start_id = kmeans_start.predict([[lat1, lon1]])[0]\n",
    "    zone_end_id = kmeans_end.predict([[lat2, lon2]])[0]\n",
    "    \n",
    "    # 7. Build feature dict (numeric features)\n",
    "    feature_dict = {\n",
    "        'Trip_Distance_km': trip_dist,\n",
    "        'Trip_Duration_min': trip_dur,\n",
    "        'Avg_Speed': payload['Avg_Speed'],\n",
    "        'Idle_Time_min': idle_time,\n",
    "        'Moving_Time_min': moving_time,\n",
    "        'Idle_Percentage': payload['Idle_Percentage'],\n",
    "        'haversine_km': haversine_km,\n",
    "        'detour_ratio': detour_ratio,\n",
    "        'bearing': bearing,\n",
    "        'delta_lat': delta_lat,\n",
    "        'delta_lon': delta_lon,\n",
    "        'lat_start': lat1,\n",
    "        'lon_start': lon1,\n",
    "        'lat_end': lat2,\n",
    "        'lon_end': lon2,\n",
    "        'hour_sin': hour_sin,\n",
    "        'hour_cos': hour_cos,\n",
    "        'dow_sin': dow_sin,\n",
    "        'dow_cos': dow_cos,\n",
    "        'idle_ratio': idle_ratio,\n",
    "        'moving_ratio': moving_ratio,\n",
    "        'idle_per_km': idle_per_km,\n",
    "        'Age_of_Truck_months': payload['Age_of_Truck_months'],\n",
    "        'vehicle_id_encoded': vehicle_id_encoded\n",
    "    }\n",
    "    \n",
    "    # 8. One-hot encode categorical (zones + distance_bucket)\n",
    "    categorical_dict = {\n",
    "        'zone_start_id': zone_start_id,\n",
    "        'zone_end_id': zone_end_id,\n",
    "        'distance_bucket': distance_bucket\n",
    "    }\n",
    "    \n",
    "    # Create temp dataframe for get_dummies\n",
    "    temp_df = pd.DataFrame([categorical_dict])\n",
    "    encoded = pd.get_dummies(temp_df, columns=['zone_start_id', 'zone_end_id', 'distance_bucket'],\n",
    "                            prefix=['zone_start_id', 'zone_end_id', 'distance_bucket'])\n",
    "    \n",
    "    # Align to training dummy columns\n",
    "    for col in dummy_columns:\n",
    "        if col not in encoded.columns:\n",
    "            encoded[col] = 0\n",
    "    encoded = encoded[dummy_columns]\n",
    "    \n",
    "    # 9. Combine numeric + categorical in exact training order\n",
    "    numeric_df = pd.DataFrame([feature_dict])[numeric_features]\n",
    "    X = pd.concat([numeric_df.reset_index(drop=True), encoded.reset_index(drop=True)], axis=1)\n",
    "    \n",
    "    # 10. Predict\n",
    "    pred = model.predict(X)[0]\n",
    "    return float(np.clip(pred, 0, 1))\n",
    "\n",
    "\n",
    "# TEST THE INFERENCE FUNCTION\n",
    "print(\"=\" * 80)\n",
    "print(\"INFERENCE FUNCTION TEST\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Use a real sample from test set\n",
    "sample_idx = 5\n",
    "sample = test_data.iloc[sample_idx]\n",
    "\n",
    "# Build payload from raw inputs (simulating API call)\n",
    "test_payload = {\n",
    "    'vehicle_id': sample['vehicle_id'],\n",
    "    'timestamp_start': sample['timestamp_start'].isoformat(),\n",
    "    'lat_start': sample['lat_start'],\n",
    "    'lon_start': sample['lon_start'],\n",
    "    'lat_end': sample['lat_end'],\n",
    "    'lon_end': sample['lon_end'],\n",
    "    'Trip_Distance_km': sample['Trip_Distance_km'],\n",
    "    'Trip_Duration_min': sample['Trip_Duration_min'],\n",
    "    'Idle_Time_min': sample['Idle_Time_min'],\n",
    "    'Moving_Time_min': sample['Moving_Time_min'],\n",
    "    'Avg_Speed': sample['Avg_Speed'],\n",
    "    'Idle_Percentage': sample['Idle_Percentage'],\n",
    "    'Age_of_Truck_months': sample['Age_of_Truck_months']\n",
    "}\n",
    "\n",
    "# Predict using inference function\n",
    "predicted = predict_route_efficiency(test_payload, loaded_artifacts)\n",
    "\n",
    "print(f\"Sample Index: {sample_idx}\")\n",
    "print(f\"Actual route_efficiency: {sample['route_efficiency']:.4f}\")\n",
    "print(f\"Predicted (via inference function): {predicted:.4f}\")\n",
    "print(f\"Absolute error: {abs(predicted - sample['route_efficiency']):.4f}\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n✓ Inference function works correctly!\")\n",
    "print(\"✓ Ready to integrate into Flask API\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
